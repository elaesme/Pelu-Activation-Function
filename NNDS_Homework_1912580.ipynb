{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNDS_Homework_1912580",
      "provenance": [],
      "collapsed_sections": [
        "wAEgygyPfO7b",
        "Rl9dZ3HambSz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnnRxTNdyW1"
      },
      "source": [
        "# Neural Networks for Data Science Applications\n",
        "## Mid-term Homework: Implementing a custom activation function\n",
        "\n",
        "**Name**: *Sabriye Ela Esme*\n",
        "\n",
        "**Matricola**: *1912580*\n",
        "\n",
        "Send the completed notebook before 26/11/2020 back to **simone.scardapane@uniroma1.it** with the object \"[NNDS] Homework_1_\\<id\\>\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEr8qV6-nMuL"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAEgygyPfO7b"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The **exponential linear unit** (ELU) is an activation function defined as [1]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\left(x\\right)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a hyper-parameter. The function is implemented in `tf.keras.layers.ELU` (see the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)).\n",
        "\n",
        "The **parametric ELU** (PELU) extends the ELU activation function as [2]:\n",
        "\n",
        "$$\n",
        "\\phi(x) =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{\\alpha}{\\beta}x & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        "\\alpha \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$\n",
        "\n",
        "where the major difference is that $\\alpha,\\beta > 0$ are *trainable* parameters, i.e., a pair of $(\\alpha, \\beta)$ values is trained for each unit in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u4aF6Z4maHd"
      },
      "source": [
        "### Exercise 1: implement the PELU\n",
        "\n",
        "In TensorFlow, it is possible to implement new layers by subclassing `tf.keras.layers.Layer`:\n",
        "\n",
        "+ [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
        "+ [Custom layers](https://www.tensorflow.org/tutorials/customization/custom_layers)\n",
        "+ [tf.keras.layers.Layer (documentation)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)\n",
        "\n",
        "**Exercise 1**: *After carefully reading the guides*, complete the following implementation of the PELU activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfCpFg2xdug_"
      },
      "source": [
        "#From the constraints(callable) page you provided us as a hint, I found out the codes for the different constraints and I realized there are some options\n",
        "#but there is no option for 'positive', So I took the NonNeg class, and I just changed the greater_equal function to greater and now I have my positive constraint\n",
        "\n",
        "class PELU(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units=32):\n",
        "        super(PELU, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    #BUILDED NON NEGATIVE TRAINABLE CONSTRAINTS\n",
        "    def build(self, input_shape):\n",
        "        # TODO: write the code here\n",
        "        self.alpha = self.add_weight(shape=(self.units,),initializer=\"ones\",trainable=True, constraint=\"non_neg\")\n",
        "        self.beta = self.add_weight(shape=(self.units,), initializer=\"ones\", trainable=True, constraint=\"non_neg\")\n",
        "\n",
        "    #USED +1e-6 AS SUGGESTED TO AVOID THE NaNS DURING TRAINING\n",
        "    def call(self, inputs):\n",
        "        newinp=tf.where(inputs>=0, (self.alpha/self.beta+1e-6)*inputs, self.alpha*(tf.exp(inputs/self.beta+1e-6)-1))\n",
        "        return newinp\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PELU, self).get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8MEZxqbn8vs"
      },
      "source": [
        "**Hints for a correct implementation**:\n",
        "\n",
        "1. The layer (probably) requires two sets of trainable variables, whose shape depends on the number of units.\n",
        "2. From the definition of the PELU, $\\alpha, \\beta$ are required to be positive in order to ensure differentiability. The simplest way to handle this is to use a [constraint callable](https://www.tensorflow.org/api_docs/python/tf/keras/constraints) when creating the weight (see also the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) for `add_weight`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QY0TfCxo9yd"
      },
      "source": [
        "### Exercise 2: some preliminary tests\n",
        "\n",
        "To evaluate your implementation, let us start by creating a single PELU function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFNFXarBe8DV"
      },
      "source": [
        "pelu = PELU(units=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWY76mEepSgj"
      },
      "source": [
        "**Exercise 2.1**: plot the function using the skeleton code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdK0CyscfDtC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "1443dbea-b1b6-4ba1-9168-c846b1344bff"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x_range = tf.dtypes.cast(tf.linspace(-5, 5, 200), tf.float32) # An equispaced grid of 200 points in [-5, +5]\n",
        "y_range = tf.dtypes.cast(pelu(x_range), tf.float32)           #THE OUTPUTS OF PELU\n",
        "\n",
        "plt.plot(np.asarray(x_range), np.asarray(y_range))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc3515bd8d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyGIlR_aqayc"
      },
      "source": [
        "The derivative of a PELU function with respect to the $\\alpha$ parameter is given by [2]:\n",
        "\n",
        "$$\n",
        "\\frac{d\\phi(x)}{d\\alpha} =\n",
        "\\Biggl\\{ \n",
        "\\begin{align} \n",
        "\\frac{x}{\\beta} & \\;\\; \\text{ if } x \\ge 0 \\\\\n",
        " \\left(\\exp\\Bigl(\\frac{x}{\\beta}\\Bigr)- 1\\right) & \\;\\; \\text{ otherwise } \n",
        "\\end{align}\n",
        "\\Bigr.\n",
        "\\,,\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnvkoRVAqwg1"
      },
      "source": [
        "**Exercise 2.2**: using a `tf.GradientTape` object, compute the derivative above using automatic differentiation, and check its correctness up to a certain numerical precision.\n",
        "\n",
        "**Hints for a correct implementation**:\n",
        "\n",
        "1. `tf.GradientTape` allows to compute the derivative *at a single point x*. If you prefer to avoid a loop over all possible points, consider using the `jacobian` function to obtain them in a single pass ([Advanced Automatic Differentiation](https://www.tensorflow.org/guide/advanced_autodiff)).\n",
        "2. Given two tensors x and y, a simple way to compute elementwise similarity up to a certain precision (e.g., $10^{-4}$), is given by `tf.reduce_all(tf.abs(x - y) < 1e-4)`.\n",
        "\n",
        "**Exercise 2.3 (optional)**: try the same for the $\\beta$ parameter (you can check the analytical formula for the gradient in the original paper [2]). **Careful**: the equation in the original paper has a missing $h$ (thanks to Davide Aureli and Federico Siciliano for spotting this). See [the correct derivation](https://www.wolframalpha.com/input/?i=d%28a*%28exp%28h%2Fb%29-1%29%29%2Fdb) on Wolfram Alpha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZM6jp0UDHyu"
      },
      "source": [
        "#MANUAL GRADIENT \n",
        "#y=pelu(x_range)\n",
        "y_grad=tf.where(x_range>=0, x_range/(pelu.beta+1e-6), tf.exp(x_range/pelu.beta+1e-6)-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AJsusZj_EVv"
      },
      "source": [
        "#TF GRADIENT \n",
        "with tf.GradientTape() as tape:\n",
        "  yy=pelu(x_range)\n",
        "\n",
        "dy_dx = tf.reshape(tape.jacobian(yy, pelu.alpha), -1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm5vfOmEC60q",
        "outputId": "54dbe862-7c98-4bcd-c9f6-30190899d752"
      },
      "source": [
        "tf.reduce_all(tf.abs(y_grad - dy_dx) < 1e-4)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlT6wQQzHEuw"
      },
      "source": [
        "#FOR BETA\n",
        "y_grad1=tf.where(x_range>=0, (-1*pelu.alpha*x_range)/(pelu.beta+1e-6)**2, (tf.exp(x_range/pelu.beta+1e-6)*-1*pelu.alpha*x_range)/(pelu.beta+1e-6)**2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMpJHcd8FB0V"
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  yy=pelu(x_range)\n",
        "\n",
        "dy_dx1 = tf.reshape(tape.jacobian(yy, pelu.beta), -1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DOviD0hFMaD",
        "outputId": "f8c34c2f-17f8-4bbb-8835-36ba2480e807"
      },
      "source": [
        "tf.reduce_all(tf.abs(y_grad1 - dy_dx1) < 1e-4)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjnVl6Ysm7F"
      },
      "source": [
        "### Exercise 3: PELU in practice\n",
        "\n",
        "Consider a simple model built with the PELU activation function, as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF5S67DDs7xr"
      },
      "source": [
        "#ADDED A FLATTEN LAYER \n",
        "model = tf.keras.Sequential(layers=[tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.Dense(50),\n",
        "      PELU(50),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvVbeTDwtdbu"
      },
      "source": [
        "**Exercise 3**: load any classification dataset, and train the model above (using either a custom training loop or `model.fit(...)`). Additionally, compare with a standard ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHubpcbwIPp2"
      },
      "source": [
        "#IMPORTED THE DATASET FROM https://www.tensorflow.org/datasets \n",
        "import tensorflow_datasets as tfds\n",
        "(dtrain, dtest), dinfo = tfds.load('mnist',split=['train', 'test'],shuffle_files=True,as_supervised=True,with_info=True,)\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "dtrain = dtrain.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dtrain = dtrain.cache()\n",
        "dtrain = dtrain.shuffle(dinfo.splits['train'].num_examples)\n",
        "dtrain = dtrain.batch(128)\n",
        "dtrain = dtrain.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "dtest = dtest.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dtest = dtest.batch(128)\n",
        "dtest = dtest.cache()\n",
        "dtest = dtest.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpHI1HfbzAQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455cafe2-04ce-467e-cf47-089216d849cf"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
        "model.fit(dtrain, epochs=50,validation_data=dtest) # TODO: Your code here"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4227 - accuracy: 0.8803 - val_loss: 0.2542 - val_accuracy: 0.9267\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.2220 - accuracy: 0.9374 - val_loss: 0.1885 - val_accuracy: 0.9441\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1623 - accuracy: 0.9523 - val_loss: 0.1472 - val_accuracy: 0.9560\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1271 - accuracy: 0.9625 - val_loss: 0.1228 - val_accuracy: 0.9629\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1049 - accuracy: 0.9686 - val_loss: 0.1119 - val_accuracy: 0.9648\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0875 - accuracy: 0.9738 - val_loss: 0.1072 - val_accuracy: 0.9681\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0760 - accuracy: 0.9769 - val_loss: 0.1082 - val_accuracy: 0.9660\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0655 - accuracy: 0.9799 - val_loss: 0.0958 - val_accuracy: 0.9702\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9830 - val_loss: 0.0961 - val_accuracy: 0.9712\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0502 - accuracy: 0.9846 - val_loss: 0.1019 - val_accuracy: 0.9710\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0983 - val_accuracy: 0.9712\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 0.0945 - val_accuracy: 0.9714\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 0.1005 - val_accuracy: 0.9734\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0306 - accuracy: 0.9903 - val_loss: 0.1061 - val_accuracy: 0.9717\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.1037 - val_accuracy: 0.9718\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0238 - accuracy: 0.9926 - val_loss: 0.1090 - val_accuracy: 0.9724\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.1073 - val_accuracy: 0.9735\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.1209 - val_accuracy: 0.9715\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.1244 - val_accuracy: 0.9711\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.1350 - val_accuracy: 0.9716\n",
            "Epoch 21/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.1346 - val_accuracy: 0.9706\n",
            "Epoch 22/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.1285 - val_accuracy: 0.9738\n",
            "Epoch 23/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.1501 - val_accuracy: 0.9711\n",
            "Epoch 24/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.1520 - val_accuracy: 0.9709\n",
            "Epoch 25/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.1466 - val_accuracy: 0.9712\n",
            "Epoch 26/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.1670 - val_accuracy: 0.9696\n",
            "Epoch 27/50\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.1633 - val_accuracy: 0.9724\n",
            "Epoch 28/50\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.1635 - val_accuracy: 0.9716\n",
            "Epoch 29/50\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.1742 - val_accuracy: 0.9713\n",
            "Epoch 30/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0102 - accuracy: 0.9963 - val_loss: 0.1690 - val_accuracy: 0.9716\n",
            "Epoch 31/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.1773 - val_accuracy: 0.9710\n",
            "Epoch 32/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.1788 - val_accuracy: 0.9703\n",
            "Epoch 33/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.1826 - val_accuracy: 0.9710\n",
            "Epoch 34/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.1709 - val_accuracy: 0.9737\n",
            "Epoch 35/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.1814 - val_accuracy: 0.9716\n",
            "Epoch 36/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.1951 - val_accuracy: 0.9699\n",
            "Epoch 37/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1860 - val_accuracy: 0.9721\n",
            "Epoch 38/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.1813 - val_accuracy: 0.9708\n",
            "Epoch 39/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.2200 - val_accuracy: 0.9688\n",
            "Epoch 40/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.1883 - val_accuracy: 0.9724\n",
            "Epoch 41/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.2005 - val_accuracy: 0.9708\n",
            "Epoch 42/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1924 - val_accuracy: 0.9711\n",
            "Epoch 43/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.1848 - val_accuracy: 0.9721\n",
            "Epoch 44/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.2080 - val_accuracy: 0.9723\n",
            "Epoch 45/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.2006 - val_accuracy: 0.9725\n",
            "Epoch 46/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.2066 - val_accuracy: 0.9719\n",
            "Epoch 47/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.2088 - val_accuracy: 0.9715\n",
            "Epoch 48/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.2021 - val_accuracy: 0.9712\n",
            "Epoch 49/50\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.2072 - val_accuracy: 0.9721\n",
            "Epoch 50/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.2139 - val_accuracy: 0.9714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc38b966908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izpZbEfiduJg"
      },
      "source": [
        "## WITH RELU ACTIVATION \n",
        "\n",
        "#ADDED A FLATTEN LAYER \n",
        "modelR = tf.keras.Sequential(layers=[tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.Dense(50),\n",
        "      tf.keras.layers.ReLU(),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi1ud1uDeCTe",
        "outputId": "2b226d42-9ba4-436d-e39e-f6492a186a7c"
      },
      "source": [
        "modelR.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "modelR.fit(dtrain,\n",
        "    epochs=50,\n",
        "    validation_data=dtest,) # TODO: Your code here"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4531 - accuracy: 0.8761 - val_loss: 0.2463 - val_accuracy: 0.9315\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.2215 - accuracy: 0.9377 - val_loss: 0.1872 - val_accuracy: 0.9463\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1743 - accuracy: 0.9500 - val_loss: 0.1595 - val_accuracy: 0.9528\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1441 - accuracy: 0.9584 - val_loss: 0.1411 - val_accuracy: 0.9573\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1228 - accuracy: 0.9642 - val_loss: 0.1259 - val_accuracy: 0.9627\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.1074 - accuracy: 0.9687 - val_loss: 0.1195 - val_accuracy: 0.9638\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0959 - accuracy: 0.9717 - val_loss: 0.1093 - val_accuracy: 0.9662\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0853 - accuracy: 0.9749 - val_loss: 0.1049 - val_accuracy: 0.9680\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0774 - accuracy: 0.9768 - val_loss: 0.1044 - val_accuracy: 0.9670\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.0990 - val_accuracy: 0.9710\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0642 - accuracy: 0.9804 - val_loss: 0.0943 - val_accuracy: 0.9715\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0591 - accuracy: 0.9822 - val_loss: 0.0956 - val_accuracy: 0.9700\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9839 - val_loss: 0.0922 - val_accuracy: 0.9723\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9859 - val_loss: 0.0969 - val_accuracy: 0.9692\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0457 - accuracy: 0.9867 - val_loss: 0.0903 - val_accuracy: 0.9732\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0421 - accuracy: 0.9880 - val_loss: 0.0885 - val_accuracy: 0.9729\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0394 - accuracy: 0.9884 - val_loss: 0.0906 - val_accuracy: 0.9745\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0362 - accuracy: 0.9896 - val_loss: 0.0884 - val_accuracy: 0.9742\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 0.9909 - val_loss: 0.0930 - val_accuracy: 0.9730\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9916 - val_loss: 0.0891 - val_accuracy: 0.9732\n",
            "Epoch 21/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0291 - accuracy: 0.9922 - val_loss: 0.0896 - val_accuracy: 0.9744\n",
            "Epoch 22/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0266 - accuracy: 0.9928 - val_loss: 0.0950 - val_accuracy: 0.9736\n",
            "Epoch 23/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.0947 - val_accuracy: 0.9733\n",
            "Epoch 24/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9940 - val_loss: 0.0953 - val_accuracy: 0.9728\n",
            "Epoch 25/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0214 - accuracy: 0.9945 - val_loss: 0.0985 - val_accuracy: 0.9723\n",
            "Epoch 26/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 0.9943 - val_loss: 0.1050 - val_accuracy: 0.9713\n",
            "Epoch 27/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.0961 - val_accuracy: 0.9745\n",
            "Epoch 28/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 0.9960 - val_loss: 0.0989 - val_accuracy: 0.9737\n",
            "Epoch 29/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0156 - accuracy: 0.9966 - val_loss: 0.0995 - val_accuracy: 0.9741\n",
            "Epoch 30/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0974 - val_accuracy: 0.9744\n",
            "Epoch 31/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.1014 - val_accuracy: 0.9737\n",
            "Epoch 32/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0121 - accuracy: 0.9976 - val_loss: 0.1005 - val_accuracy: 0.9743\n",
            "Epoch 33/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.1051 - val_accuracy: 0.9726\n",
            "Epoch 34/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0110 - accuracy: 0.9977 - val_loss: 0.1067 - val_accuracy: 0.9740\n",
            "Epoch 35/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0099 - accuracy: 0.9984 - val_loss: 0.1098 - val_accuracy: 0.9745\n",
            "Epoch 36/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0090 - accuracy: 0.9984 - val_loss: 0.1086 - val_accuracy: 0.9740\n",
            "Epoch 37/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.1161 - val_accuracy: 0.9717\n",
            "Epoch 38/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0081 - accuracy: 0.9987 - val_loss: 0.1109 - val_accuracy: 0.9745\n",
            "Epoch 39/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.1204 - val_accuracy: 0.9727\n",
            "Epoch 40/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.1146 - val_accuracy: 0.9740\n",
            "Epoch 41/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.1215 - val_accuracy: 0.9728\n",
            "Epoch 42/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0066 - accuracy: 0.9989 - val_loss: 0.1198 - val_accuracy: 0.9736\n",
            "Epoch 43/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.1226 - val_accuracy: 0.9741\n",
            "Epoch 44/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.1213 - val_accuracy: 0.9731\n",
            "Epoch 45/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9996 - val_loss: 0.1225 - val_accuracy: 0.9743\n",
            "Epoch 46/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.1284 - val_accuracy: 0.9733\n",
            "Epoch 47/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.1218 - val_accuracy: 0.9747\n",
            "Epoch 48/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.1322 - val_accuracy: 0.9733\n",
            "Epoch 49/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.1284 - val_accuracy: 0.9737\n",
            "Epoch 50/50\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.1315 - val_accuracy: 0.9737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc334527c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQsMBwPEJGKF"
      },
      "source": [
        "###FINAL RESULTS OF PELU:  train_accuracy: 0.9985 - val_accuracy: 0.9714\n",
        "###FINAL RESULTS OF RELU:  train_accuracy: 0.9997 - val_accuracy: 0.9737"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkb3cj9r7uUH"
      },
      "source": [
        "### Optional: understanding saving/loading of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjaJQT06jh9"
      },
      "source": [
        "TensorFlow has several options for saving or loading objects from the disk:\n",
        "\n",
        "1. [Save and load Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize/)\n",
        "\n",
        "In many cases, custom classes require the implementation of a `get_config` / `from_config` functions to define the serialization behaviour.\n",
        "\n",
        "**Exercise 4 (optional)**: implement the `get_config` method and test your implementation as below (taken from the guide on saving and loading models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzyPwvvJMoAT"
      },
      "source": [
        "###TRIAL FOR 4TH QUESTION, GETS ERROR\n",
        "config = model.get_config()\n",
        "print(config)\n",
        "custom_objects = {\"PELU\": PELU}\n",
        "with keras.utils.custom_object_scope(custom_objects):\n",
        "    new_model = keras.Model.from_config(config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRIJFcGbMzex"
      },
      "source": [
        "print(\"Original model:\", model)\n",
        "print(\"Loaded model:\", reloaded_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voxx7kntvM0W"
      },
      "source": [
        "model.save('/pelu_model')\n",
        "del PELU # This is needed to remove any reference to PELU from memory\n",
        "reloaded_model = tf.keras.models.load_model('/pelu_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "838PN8xN0rIW"
      },
      "source": [
        "print(\"Original model:\", model)\n",
        "print(\"Loaded model:\", reloaded_model) # Observe that the object has been dynamically recreated in absence of the configuration options"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9dZ3HambSz"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] Clevert, D.A., Unterthiner, T. and Hochreiter, S., 2015. [Fast and accurate deep network learning by exponential linear units (ELUs)](https://arxiv.org/abs/1511.07289). arXiv preprint arXiv:1511.07289.\n",
        "\n",
        "[2] Trottier, L., Gigu, P. and Chaib-draa, B., 2017. [Parametric exponential linear unit for deep convolutional neural networks](https://arxiv.org/abs/1605.09332). In 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA) (pp. 207-214). IEEE."
      ]
    }
  ]
}